#include <stdio.h>
#include <iostream>
#include "CNTKLibrary.h"
#include <time.h>
#include <cuda_runtime.h>
#include <cuda_runtime_api.h>
#include <cuda.h>
#include <device_launch_parameters.h>
#include <sys/ipc.h>
#include <sys/shm.h>

extern "C"{
  #include "onvm_cntk_api.h"
}

using namespace CNTK;

//these modules will be shared so they can be global variables 
FunctionPtr rootFunc_cpu[NUMBER_OF_MODELS], rootFunc_gpu[NUMBER_OF_MODELS];

/* this function loads the model and gives back the pointer */
int load_model(const wchar_t * file_path, void ** cpu_function_pointer, void ** gpu_function_pointer, int load_flag, int model_no)//, void **cpu_device_descriptor, void ** gpu_device_){
{
  setlocale(LC_ALL, "en_US.UTF-8");

  //CNTK::DeviceDescriptor cpu_device, gpu_device;

  //try loading the model
  try{
    //load the file to this variable
    switch(load_flag)
      {
      case 0:
	//rootFunc_cpu = Function::Load(file_path, cpu_device);
	rootFunc_cpu[model_no] = Function::Load(file_path, CNTK::DeviceDescriptor::CPUDevice());
	*cpu_function_pointer = (void *)(rootFunc_cpu[model_no].get());
	break;
      case 1:
	//auto& gpu_device = CNTK::DeviceDescriptor::GPUDevice(0);
	rootFunc_gpu[model_no] = Function::Load(file_path, CNTK::DeviceDescriptor::GPUDevice(0));
	*gpu_function_pointer = (void *) (rootFunc_gpu[model_no].get());
	break;
      case 2:
	rootFunc_cpu[model_no] = Function::Load(file_path, CNTK::DeviceDescriptor::CPUDevice());
	rootFunc_gpu[model_no] = Function::Load(file_path, CNTK::DeviceDescriptor::GPUDevice(0));
	*cpu_function_pointer = (void *) (rootFunc_cpu[model_no].get());
	*gpu_function_pointer = (void *) (rootFunc_gpu[model_no].get());
	break;
      default:
	break;

      }
    std::cout<<"Loaded the file \n";
    return 0;
  }
  catch(...){
    printf("Loading the file failed \n");
    return -1;
  }
  
}

/* gives the number of parameters in a ML function */
int count_parameters(void * gpu_function_pointer){
  Function *rootFunc = (Function *)gpu_function_pointer;
  int counter = 0;
  auto parameters = (rootFunc)->Parameters();
  for(auto& p : parameters){
    counter++;
  }
  std::cout<<"Number of parameters "<<counter<<std::endl;
  return counter;
}


/* this function extracts the GPU pointers and returns GPU cuda handles */
void get_cuda_pointers_handles(void *gpu_function_pointer, void *mem_handles){
  /* we do not need the number of mem handles as the user should know it or we can just simply iterate through it */
  //int number_of_handles_needed = count_parameters(gpu_function_pointer);
  //cudaIpcMemHandle_t *handles = (cudaIpcMemHandle_t*) malloc(sizeof(cudaIpcMemHandle_t)*number_of_handles_needed);

  /* also the cudaIpcMemHandle_t will be allocated by NFs*/
  cudaIpcMemHandle_t *handles = (cudaIpcMemHandle_t*)mem_handles;
  /* now need to extract the pointers from the GPU */
  Function *rootFunc = (Function*)gpu_function_pointer;
  int counter = 0;
  auto parameters = (rootFunc)->Parameters();

  for(auto& p : parameters){
    //auto gpu_ndarray = p.GetValue();
    cudaError_t cuda_error = cudaIpcGetMemHandle(&(handles[counter]), const_cast<float *>(p.GetValue()->DataBuffer<float>()));
    //check if the CUDA process went okay
    if(cuda_error != cudaSuccess)
      std::cout<<"CUDA ERROR : "<<cudaGetErrorString(cuda_error)<<std::endl;

    //debug
    std::cout<<"GPU pointer "<<p.GetValue()->DataBuffer<float>()<<std::endl;
    counter++;
  }

  //return (void *) handles;
}


/* function to attach the GPU pointers with the models */
int link_gpu_pointers(void * cpu_function_pointer, void * cuda_handles_for_gpu_data, int number_of_parameters){
  /* we assume the models are identical in CPU and GPU side */
  const auto& gpu_device = CNTK::DeviceDescriptor::GPUDevice(0);
  Function *rootFunc = (Function*) cpu_function_pointer;
  auto parameters = (rootFunc)->Parameters();
  int counter = 0;

  // a single variable should be enough
  void ** gpu_pointer = (void **)malloc(sizeof(void *)*number_of_parameters);
  
  //since the number of paramters and the number of gpu pointers are the same... we can resolve the GPU pointers in the loop itself 
  for(auto& p: parameters){
    auto ndarray = p.GetValue();
    // resolve the cuda handles
    cudaError_t cuda_error = cudaIpcOpenMemHandle((void **)&(gpu_pointer[counter]), ((cudaIpcMemHandle_t*)cuda_handles_for_gpu_data)[counter],cudaIpcMemLazyEnablePeerAccess);
    if(cuda_error != cudaSuccess)
      std::cout<<"CUDA ERROR : "<<cudaGetErrorString(cuda_error)<<" gpu pointer "<<gpu_pointer[counter]<<std::endl;

    //else
    //  std::cout<<"CUDA pointer successfully converted "<<gpu_pointer[counter]<<std::endl;

    
    //now attach the gpu pointers
    (ndarray.get())->Attach_GPU_Address(gpu_device, (float *) gpu_pointer[counter]);
    counter++;
  }
  free(gpu_pointer);
  return 0;
}


/* evaluate a data set */
// let's hard code 
void evaluate_in_gpu_input_from_host(float *input, size_t input_size, float *output, void *function_pointer, float* evaluation_time, int cuda_event_flag){
  //convert to function
  Function *rootFunc = (Function *) function_pointer;
  // get the input and output
  Variable outputvar = rootFunc->Output();

  std::vector<Variable> inputs = rootFunc->Arguments();

  //create a new vector
  Variable inputvar = inputs[0];
  long total_size = 1;

  for( auto inputvars : inputs){
    /* now create a new input */
    NDShape inputShape = inputvars.Shape();
    std::vector<size_t> inputShapeDim = inputShape.Dimensions();

    //std::cout<<"Input Dimensions "<<std::endl;
    //long total_size = 1;
    for(auto a : inputShapeDim){
      //std::cout<<"dimensions "<<a<<std::endl;
      total_size *= a;
    }
  }
  std::vector<float> inputData(input, input+input_size); //allocating a vector
  //allocate a new vector... to feed in the data..
  //for(int i =0; i<total_size;i++){
  //  inputData.at(i) = (float) (i%255);
  //}

  struct timespec begin, end;
  clock_gettime(CLOCK_MONOTONIC, &begin);
  const auto& gpu_device = CNTK::DeviceDescriptor::GPUDevice(0);
    
  ValuePtr inputVal = Value::CreateBatch(inputvar.Shape(), inputData, gpu_device);
  std::unordered_map<Variable, ValuePtr> inputDataMap = { { inputvar, inputVal } };

  // Alternatively, create a Value object and add it to the data map.
  std::unordered_map<Variable, ValuePtr> outputDataMap = { { outputvar, nullptr } };

  //creating cuda events
  cudaEvent_t start, stop;
  cudaEventCreate(&start);
  cudaEventCreate(&stop);

  /*the execution block */
  cudaEventRecord(start);
  rootFunc->Evaluate(inputDataMap, outputDataMap, gpu_device);
  cudaEventRecord(stop);
  
  ValuePtr outputVal = outputDataMap[outputvar];

  //std::cout<<"output var shape "<<outputvar.Shape().TotalSize()<<std::endl;
  auto output_ndarray = outputVal->Data()->DataBuffer<float>();

  
  cudaMemcpy(output, output_ndarray, sizeof(float)*1000, cudaMemcpyDeviceToHost);
  
  clock_gettime(CLOCK_MONOTONIC, &end);

  //if(cuda_event_flag)
  //  cudaEventSynchronize(stop);
  
  float cuda_time = 0;
  cudaEventElapsedTime(&cuda_time, start, stop);
  double time_taken = (end.tv_sec-begin.tv_sec)*1000000.0+(end.tv_nsec-begin.tv_nsec)/1000.0;
  //std::cout<<"time taken to run this module is "<<time_taken<<" micro-seconds ... and event time "<<cuda_time<<std::endl;
  std::cout<<""<<time_taken<<","<<cuda_time<<std::endl;
  //for(int k = 0; k<1000; k++)
  //  std::cout<<result[k]<<" ";

  //std::cout<<std::endl;
  //free(result);
  
}


int count_inputs(void *function_pointer){
  //convert to function
  Function *rootFunc = (Function *) function_pointer;
  std::vector<Variable> inputs = rootFunc->Arguments();
  return inputs.size();
}  


void get_all_input_sizes(void *function_pointer,int *input_dim, int input_var_number){
  Function *rootFunc = (Function *) function_pointer;
  std::vector<Variable> inputs = rootFunc->Arguments();
  int counter = 0;
  auto input = inputs.at(input_var_number);
  
  NDShape inputShape = input.Shape();
  std::vector<size_t> inputShapeDim = inputShape.Dimensions();
  for(auto a: inputShapeDim){
    input_dim[counter++] = a;
  }
}
