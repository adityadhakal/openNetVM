#include "logger.h"
#include "common.h"
#include "argsParser.h"
#include "buffers.h"

#include "NvCaffeParser.h"
#include "NvInfer.h"

#include "cudaWrapper.h"
#include "ioHelper.h"
#include "tensorrt_api.h"


#include <cuda_runtime_api.h>
#include <algorithm>
#include <cassert>
#include <cmath>
#include <fstream>
#include <iostream>
#include <sstream>
#include <string>

extern "C"{
#include "tensorrt_api.h"
#include "onvm_images.h"
}

using namespace cudawrapper;


// important file wide variables
ICudaEngine* engine{nullptr};
Logger gLogger;
// Declaring execution context.
unique_ptr<IExecutionContext, Destroy<IExecutionContext>> context{nullptr};
cudaStream_t stream; //single stream for now

//GPU side space to store.
void* bindings[2]{0};


/*--------------------------- end logging ----------------------- */
// Number of times we run inference to calculate average time.
constexpr int ITERATIONS = 10;
/*
class Logger : public ILogger
{
  void log(Severity severity, const char* msg) override
  {
    // suppress info-level messages
    if (severity != Severity::kINFO)
      std::cout << msg << std::endl;
  }
} gLogger;
*/


// Returns empty string iff can't read the file
string readBuffer(string const& path)
{
  string buffer;
  ifstream stream(path.c_str(), ios::binary);

  if (stream)
    {
      stream >> noskipws;
      copy(istream_iterator<char>(stream), istream_iterator<char>(), back_inserter(buffer));
    }

  return buffer;
}
static int getBindingInputIndex(IExecutionContext* context)
{
  return !context->getEngine().bindingIsInput(0); // 0 (false) if bindingIsInput(0), 1 (true) otherwise
}

void launchInference(IExecutionContext* context, cudaStream_t stream, float* inputTensor, int inputTensorSize, float* outputTensor, int outputTensorSize,void** bindings, int batchSize)
{
  int inputId = getBindingInputIndex(context);

  cudaMemcpyAsync(bindings[inputId], inputTensor, inputTensorSize* sizeof(float), cudaMemcpyHostToDevice, stream);
  //cudaMemcpy(bindings[inputId], inputTensor.data(), inputTensor.size() * sizeof(float), cudaMemcpyHostToDevice);
  context->enqueue(batchSize, bindings, stream, nullptr);
  cudaMemcpyAsync(outputTensor, bindings[1 - inputId], outputTensorSize * sizeof(float), cudaMemcpyDeviceToHost, stream);
  //cudaMemcpy(outputTensor.data(), bindings[1 - inputId], outputTensor.size() * sizeof(float), cudaMemcpyDeviceToHost);
}

//old function to launch inference
void launchInference_old(IExecutionContext* context, cudaStream_t stream, vector<float> const& inputTensor, vector<float>& outputTensor, void** bindings, int batchSize)
{
  int inputId = getBindingInputIndex(context);

  cudaMemcpyAsync(bindings[inputId], inputTensor.data(), inputTensor.size() * sizeof(float), cudaMemcpyHostToDevice, stream);
  //cudaMemcpy(bindings[inputId], inputTensor.data(), inputTensor.size() * sizeof(float), cudaMemcpyHostToDevice);
  context->enqueue(batchSize, bindings, stream, nullptr);
  cudaMemcpyAsync(outputTensor.data(), bindings[1 - inputId], outputTensor.size() * sizeof(float), cudaMemcpyDeviceToHost, stream);
  //cudaMemcpy(outputTensor.data(), bindings[1 - inputId], outputTensor.size() * sizeof(float), cudaMemcpyDeviceToHost);
}


void doInference(IExecutionContext* context, cudaStream_t stream, vector<float> const& inputTensor, vector<float>& outputTensor, void** bindings, int batchSize)
{
  CudaEvent start;
  CudaEvent end;
  double totalTime = 0.0;
  struct timespec begin,finish;
  clock_gettime(CLOCK_MONOTONIC, &begin);
  int temp_batchsize = 1;
  for (int i = 0; i < ITERATIONS; ++i)
    {
      float elapsedTime;

      // Measure time it takes to copy input to GPU, run inference and move output back to CPU.
      cudaEventRecord(start, stream);
      launchInference_old(context, stream, inputTensor, outputTensor, bindings, batchSize);
      //launchInference(context, stream, inputTensor, outputTensor, bindings, temp_batchsize);
      cudaEventRecord(end, stream);

      // Wait until the work is finished.
      cudaStreamSynchronize(stream);
      cudaEventElapsedTime(&elapsedTime, start, end);

      totalTime += elapsedTime;
    }
  clock_gettime(CLOCK_MONOTONIC, &finish);

  double time_elapsed = (finish.tv_sec-begin.tv_sec)*1000.0+(finish.tv_nsec-begin.tv_nsec)/1000000.0;

  cout << "Inference batch size " << batchSize << " average over " << ITERATIONS << " runs is " << totalTime / ITERATIONS << "ms" << endl;
  std::cout<<" Inference of batch of "<< temp_batchsize << " number of iterations "<<ITERATIONS <<" total images "<< temp_batchsize*ITERATIONS<<" Time "<<time_elapsed<<" ms"<<" Throughput "<<(temp_batchsize*ITERATIONS)*1000/time_elapsed<<endl;
}


int tensorrt_init(const char * file_path, int max_batchsize)
{
  //  string buffer = readBuffer("my_engine.trt");
  string buffer = readBuffer(file_path);
  std::cout<<"Buffer size "<<buffer.size()<<std::endl;

  if (buffer.size())
    {
      // try to deserialize engine
      unique_ptr<IRuntime, Destroy<IRuntime>> runtime{createInferRuntime(gLogger)};
      engine = runtime->deserializeCudaEngine(buffer.data(), buffer.size(), nullptr);
    }

  //not necessary as NF supplies them
  vector<float> inputTensor;
  vector<float> outputTensor;

  //CudaStream stream;
  
  int batchSize = max_batchsize;

  // Assume networks takes exactly 1 input tensor and outputs 1 tensor.
  assert(engine->getNbBindings() == 2);
  assert(engine->bindingIsInput(0) ^ engine->bindingIsInput(1));

  for (int i = 0; i < engine->getNbBindings(); ++i)
    {
      Dims dims{engine->getBindingDimensions(i)};
      size_t size = accumulate(dims.d, dims.d + dims.nbDims, batchSize, multiplies<size_t>());
      // Create CUDA buffer for Tensor.
      cudaMalloc(&bindings[i], size * sizeof(float));

      // Resize CPU buffers to fit Tensor.
      
      if (engine->bindingIsInput(i))
	inputTensor.resize(size);
      else
	outputTensor.resize(size);
      
    }

  cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);
  // Create Execution Context.
  context.reset(engine->createExecutionContext());
  doInference(context.get(), stream, inputTensor, outputTensor, bindings, batchSize);
  return 0;
}

//aditya's function accessible from applications
void evaluate_in_gpu_input_from_host_tensorrt(float *input, size_t input_size, float *output, size_t output_size,void *evaluation_time, int cuda_event_flag, cudaHostFn_t callback_function, void *callback_data, IExecutionContext* context, cudaStream_t stream, int batchsize){
  /*
  CudaEvent start;
  CudaEvent end;
  */
   double totalTime = 0.0;
   struct timespec *timestamps = (struct timespec *)evaluation_time;
  clock_gettime(CLOCK_MONOTONIC, &timestamps[0]);
  float elapsedTime;
  struct gpu_callback *callback_info = (struct gpu_callback *)callback_data;

  //create the events
  cudaEventCreate(&(callback_info->start_event));
  cudaEventCreate(&(callback_info->end_event));

  // Measure time it takes to copy input to GPU, run inference and move output back to CPU.
  //cudaEventRecord(start, stream);
  //launchInference(context, stream, inputTensor, outputTensor, bindings, batchSize);
  clock_gettime(CLOCK_MONOTONIC, &timestamps[1]);
  cudaEventRecord(callback_info->start_event,stream);
  launchInference(context, stream, input, input_size, output, output_size, bindings, batchsize);
  cudaEventRecord(callback_info->end_event, stream);
  
  if(callback_function !=NULL){
    cudaLaunchHostFunc(stream, callback_function, callback_data);
  }

  // Wait until the work is finished.
  //cudaStreamSynchronize(stream);
  //cudaEventElapsedTime(&elapsedTime, start, end);
  
  //totalTime += elapsedTime;
  //double time_elapsed = (finish.tv_sec-begin.tv_sec)*1000.0+(finish.tv_nsec-begin.tv_nsec)/1000000.0;

  //  cout << "Inference batch size " << batchSize << " average over " << ITERATIONS << " runs is " << totalTime / ITERATIONS << "ms" << endl;
  //std::cout<<" Inference of batch of "<< temp_batchsize << " number of iterations "<<ITERATIONS <<" total images "<< temp_batchsize*ITERATIONS<<" Time "<<time_elapsed<<" ms"<<" Throughput "<<(temp_batchsize*ITERATIONS)*1000/time_elapsed<<endl;

}


void evaluate_image_in_gpu_tensorrt(void *image, cudaHostFn_t callback_function, void *callback_data, int gpu_barrier_flag){

  //call the evaluating function.. just extract the image data here
  image_data *ready_image = (image_data *) image;
  evaluate_in_gpu_input_from_host_tensorrt(ready_image->image_data_arr, ready_image->num_data_points_stored, ready_image->output, ready_image->output_size, &(ready_image->timestamps[2]), gpu_barrier_flag, callback_function, callback_data, context.get(),stream, ready_image->batch_size);
}
